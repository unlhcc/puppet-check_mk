#!/usr/bin/env python
# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4

import errno, json, os, urllib

# Show all datanode JMX data
# curl http://localhost:50075/jmx

# Nagios exit status
STATE_OK = 0
STATE_WARNING = 1
STATE_CRITICAL = 2
STATE_UNKNOWN = 3

def main():

    # If we find a data directory, assume we should be running the datanode service
    datadir = '/hadoop-data1'

    try:
        os.stat(datadir)
    except OSError, e:
        # A disk failure returns EIO on stat()
        if e.errno != errno.EIO:
            return

    print check_datanode()

def check_datanode():
    url = 'http://localhost:50075/jmx?qry=Hadoop:service=DataNode,name=DataNodeActivity*'

    try:
        response = urllib.urlopen(url)
        data = json.loads(response.read())
    except IOError:
        return '%s HDFS_DataNode - Could not contact datanode service' % (STATE_UNKNOWN)

    vfail = data['beans'][0]['VolumeFailures']
    mfail = check_datanode_mounts()

    if vfail > 1 or mfail > 1:
        return '%s HDFS_DataNode - %s volumes failed, %s mounts failed' % (STATE_CRITICAL, vfail, mfail)
    elif vfail > 0 or mfail > 0:
        return '%s HDFS_DataNode - %s volume failed, %s mount failed' % (STATE_WARNING, vfail, mfail)
    else:
        return '%s HDFS_DataNode - Volumes OK' % (STATE_OK)

def check_datanode_mounts():
    # Check the status of the /hadoop-data* mounts
    # If XFS has an error, mountinfo will look fine, but access attempts show
    #   cannot access /hadoop-dataX: Input/output error
    import glob
    bad = 0

    for datadir in glob.glob('/hadoop-data*'):
        try:
            os.stat(datadir)
        except OSError, e:
            bad += 1

    return bad

if __name__ == "__main__":
    #runs script as main
    main()
